---
title: "Supervised ML problems"
output: html_notebook
---

# KNN regression

1. Generate $n=200$ paired $(x_i, y_i)$ data points by sampling:

\begin{equation}
x_i \sim \mathcal{N}(0, 4)
\end{equation}

and then:

\begin{equation}
y_i \sim \mathcal{N}(sin(x_i), 0.2)
\end{equation}

Visualise your samples.
```{r}
rm(list=ls())
library(tidyverse)
library(reshape2)
library(e1071)
library(RANN)

n <- 200
x <- rnorm(n, 0, 4)
y <- sin(x) + rnorm(n, 0, 0.2)

df <- tibble(x, y) %>% 
  mutate(type="actual")

ggplot(df %>% filter(type=="actual"), aes(x=x, y=y)) +
  geom_point()
```

2. Write a function that returns the Euclidean distance of each point from each other point.

Note: both R and Python have functions for calculating distance matrices:
- R: `dist`
- Python: `scipy.spatial.distance_matrix`

```{r}
distance <- function(df) {
  d <- as.matrix(dist(df[1:5, 1:2]))
  diag(d) <- Inf
  d
}
```

3. Write a function which returns the mean of the K-nearest neighbour points for all points (excluding itself of course). This is your KNN regression function.
```{r}
knn <- function(k, df) {
  d <- distance(df)
  
  # Get indices of knn for each point
  m_indices <- matrix(nrow = nrow(df),
                      ncol = k)
  for(i in 1:nrow(d)) {
    low_to_high_indices <- order(d[i, ])
    m_indices[i, ] <- low_to_high_indices[1:k]
  }
  
  # Get KNN points for each point
  x <- df[, 1:2]
  v_means <- vector(length=nrow(df))
  for(i in seq_along(v_means)) {
    points <- x[m_indices[i, ], ]
    v_means[i] <- colMeans(points)
  }
  v_means
}

```


2. Create a function which returns the KNNs for a given $x$ value (and $k$)
```{r}
# use RANN library (which implements KD trees)
x_tilde <- -2.5
fit <- nn2(x, c(x_tilde), k = 10)
idxs <- fit$nn.idx[1, ]

# visualise nearest neighbours around a point
tibble(x, y) %>% 
  mutate(idx=seq_along(x)) %>% 
  mutate(nn=idx%in%idxs) %>% 
  ggplot(aes(x=x, y=y, colour=as.factor(nn))) +
  geom_point()
```

3. Using your answer to the previous code, create a KNN regression estimator using Euclidean distance with various values of $k$ to predict $y$. How does $k$ influence the results?
```{r}
f_kk_regression <- function(x_tilde, x, y, k=10) {
  fit <- nn2(x, c(x_tilde), k = k)
  idxs <- fit$nn.idx[1, ]
  return(mean(y[idxs]))
}

x_sim <- seq(-10, 10, 0.1)

# k = 1
y_sim <- map_dbl(x_sim, ~f_kk_regression(., x, y, k=10))

df <- tibble(x, y) %>% 
  mutate(type="actual") %>% 
  bind_rows(tibble(x=x_sim, y=y_sim) %>% mutate(type="regression"))

ggplot(df %>% filter(type=="actual"), aes(x=x, y=y)) +
  geom_point() +
  geom_line(data=df %>% filter(type=="regression"), colour="blue")

# k = 1
y_sim <- map_dbl(x_sim, ~f_kk_regression(., x, y, k=1))

df <- tibble(x, y) %>% 
  mutate(type="actual") %>% 
  bind_rows(tibble(x=x_sim, y=y_sim) %>% mutate(type="regression"))

ggplot(df %>% filter(type=="actual"), aes(x=x, y=y)) +
  geom_point() +
  geom_line(data=df %>% filter(type=="regression"), colour="blue")

# k = 100
y_sim <- map_dbl(x_sim, ~f_kk_regression(., x, y, k=100))

df <- tibble(x, y) %>% 
  mutate(type="actual") %>% 
  bind_rows(tibble(x=x_sim, y=y_sim) %>% mutate(type="regression"))

ggplot(df %>% filter(type=="actual"), aes(x=x, y=y)) +
  geom_point() +
  geom_line(data=df %>% filter(type=="regression"), colour="blue")
```

